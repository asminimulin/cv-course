{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/asminimulin/cv-course/blob/main/02-image-linear-classification/Linear_classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BlRBhMZtdf94"
      },
      "source": [
        "# Download dataset with Pytorch\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BNE5Qqw_rdsV"
      },
      "source": [
        "## Load the data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from functools import partial\n",
        "from torch import Tensor\n",
        "import torch\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "zi8Iull1QkqB"
      },
      "execution_count": 235,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vKAZ-aQtrg8q"
      },
      "source": [
        "from torchvision import models, datasets, transforms\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "# Define transformation for each image\n",
        "transform  = transforms.Compose([\n",
        "    transforms.Lambda(lambda x: np.array(x).flatten().astype(np.float32) / 255) #Stretch image into row [32,32,3] -> [3072]\n",
        "])\n",
        "\n",
        "# Download a CIFAR10 dataset\n",
        "dataset = datasets.CIFAR10(\"/content\",\n",
        "                           train=True,\n",
        "                           transform = transform,\n",
        "                           download=False)\n"
      ],
      "execution_count": 366,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Make sure there is no problems with dtype overflow"
      ],
      "metadata": {
        "id": "Xz6QQrrHvjk5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset[0][0].dtype)"
      ],
      "metadata": {
        "id": "Q3mkBqnqvt3F",
        "outputId": "a9e06bbc-514c-4bcf-8815-f5bbe9960e5c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 367,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "float32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I3F4IoDErlcw"
      },
      "source": [
        "## Split dataset & define dataloaders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fu02DmABYxoh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "8cc34d82-bd9c-42cd-8acd-44d981a2f930"
      },
      "source": [
        "train_ds, val_ds, _= random_split(dataset, [25000,10000 ,15000])\n",
        "# Hint: Perform debug on smaller subset\n",
        "\n",
        "batch_size = 500\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size = batch_size)\n",
        "val_loader = DataLoader(val_ds, batch_size = 15000)\n",
        "\n",
        "# Display one image\n",
        "for images, class_nums in train_loader:\n",
        "  print (images.shape,class_nums.shape) # class_nums are tensor!\n",
        "  grayscale = np.uint8(images[0].reshape((32, 32, 3)).numpy() * 255)\n",
        "  display(Image.fromarray(grayscale, 'RGB'),class_nums[0].item()) \n",
        "  break\n"
      ],
      "execution_count": 401,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([500, 3072]) torch.Size([500])\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=32x32 at 0x7FD4FB902CD0>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GqC6xXhSeUPr"
      },
      "source": [
        "# Implement LinearClassifier class for CIFAR10"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vaxIJUM7eQUp"
      },
      "source": [
        "class LinearClassifier:\n",
        "  def __init__(self, labels):\n",
        "    self.__labels = labels\n",
        "    self.__classes_num = len(labels)\n",
        "    # self.__weights = torch.from_numpy((np.random.randn(1024, self.__classes_num) * 0.0001).astype(np.float32))\n",
        "    self.__w_unnormalized = torch.randn(1025, len(labels), dtype=torch.float32, requires_grad=True)\n",
        "    self.__weights = self.__w_unnormalized * 0.0001\n",
        "\n",
        "  \n",
        "  def train(self, images: np.array, labels: np.array, learning_rate = 1e-6):\n",
        "    \"\"\"\n",
        "      Arguments:\n",
        "        images  (numpy.array): collection of objects (batch)\n",
        "        labels  (numpy.array): collection of integer \n",
        "        representing a class number for objects from x\n",
        "    \n",
        "    \"\"\"\n",
        "    images = torch.from_numpy(self.__add_ones(images.numpy()).astype(np.float32))\n",
        "    # images = torch.from_numpy(images)\n",
        "\n",
        "    loss_val, grad = self.__loss(images, labels)\n",
        "\n",
        "    self.__weights -= (grad * learning_rate)\n",
        "\n",
        "    return loss_val/images.shape[0]\n",
        "\n",
        "\n",
        "  def predict(self,x):\n",
        "    # x = torch.from_numpy(x.astype(np.float32))\n",
        "    x = torch.from_numpy(self.__add_ones(x).astype(np.float32))\n",
        "    scores = x @ self.__weights # (256, 1025) * (1025, 10)\n",
        "    return torch.argmax(scores,axis = 1)\n",
        "\n",
        "\n",
        "  __weights: np.array  # np.array of shape (1024, 10)\n",
        "\n",
        "\n",
        "  def __loss(self, images, labels):\n",
        "    \"\"\"\n",
        "      Arguments:\n",
        "        images  (numpy.array or torch.Tensor): collection of objects (batch)\n",
        "        labels  (numpy.array or torch.Tensor): collection of integer \n",
        "        representing a class number for objects from x\n",
        "    \n",
        "    \"\"\"\n",
        "\n",
        "    # Calculate Multiclass SVM or Cross-entropy loss over a batch \n",
        "    # loss = self.__calc_svm_loss(images, labels, self.__weights)\n",
        "    loss = self.__calc_mce_loss(images, labels, self.__weights)\n",
        "\n",
        "    # Calculate gradients (dL/dW) and store it in dW\n",
        "    # grad = self.__calc_numeric_grad(partial(self.__calc_svm_loss, images, labels), loss)\n",
        "    loss.backward(retain_graph=True)\n",
        "    grad = self.__w_unnormalized.grad\n",
        "\n",
        "    return loss, grad\n",
        "\n",
        "\n",
        "  @staticmethod\n",
        "  def __add_ones(images):\n",
        "    return np.append(images, np.ones((images.shape[0], 1)), axis=1)\n",
        "\n",
        "\n",
        "  @staticmethod\n",
        "  def __calc_svm_loss(images: np.array, labels: np.array, weights: np.array):\n",
        "    # images.shape = (N, S)\n",
        "    # self.__wights.shape = (S, 10)\n",
        "    # (N, S) * (S, 10) = (N, 10)\n",
        "    # scores is two-dimensional np.array, each row contains scores of every class\n",
        "    scores = images @ weights\n",
        "    \n",
        "    correct_scores = np.zeros((scores.shape[0], 1))\n",
        "    for i, (label, score_arr) in enumerate(zip(labels, scores)):\n",
        "      correct_scores[i][0] = score_arr[label] + 1\n",
        "\n",
        "    diffs = correct_scores - scores\n",
        "    np.clip(diffs, 0, max(0, np.max(diffs)), out=diffs)\n",
        "    return np.sum(diffs)\n",
        "\n",
        "\n",
        "  @staticmethod\n",
        "  def __calc_mce_loss(images: Tensor, labels: Tensor, weights: Tensor):\n",
        "    scores = images @ weights\n",
        "\n",
        "    # print(scores[:5])\n",
        "    scores_exp = torch.exp(scores)\n",
        "    # print(scores_exp[:5])\n",
        "    scores_sums = torch.sum(scores_exp, dim=1, keepdim=True)\n",
        "    # print(scores_sums[:5])\n",
        "    scores_probabilities = scores_exp / scores_sums\n",
        "    # print(scores_probabilities[:5])\n",
        "    # print(torch.sum(scores_probabilities, dim=1)[:5])\n",
        "\n",
        "    return (-torch.log(torch.gather(scores_probabilities, dim=1, index=labels.reshape((labels.shape[0], 1))))).sum()\n",
        "    # return scores.sum()\n",
        "\n",
        "\n",
        "  def __cals_analytic_grad(self):\n",
        "    pass\n",
        "\n",
        "  def __calc_numeric_grad(self, calc_loss, current_loss: float, delta=0.0001):\n",
        "    grad = [[0] * self.__weights.shape[1]] * self.__weights.shape[0]\n",
        "    for i in range(self.__weights.shape[0]):\n",
        "      for j in range(self.__weights.shape[1]):\n",
        "        d_weights = np.copy(self.__weights)\n",
        "        d_weights[i, j] += delta\n",
        "        loss_diff = calc_loss(d_weights) - current_loss\n",
        "        grad[i][j] = (loss_diff) / delta\n",
        "    return np.array(grad)\n"
      ],
      "execution_count": 370,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def my_test():\n",
        "  # print(dataset.classes)\n",
        "  # for images, class_nums in train_loader:\n",
        "  #   print(len(images), len(class_nums))\n",
        "  #   print(class_nums.shape)\n",
        "  #   return\n",
        "  #   print(class_nums[:15], min(class_nums), max(class_nums))\n",
        "  #   for tensor, label in zip(images[:15], class_nums[:15]):\n",
        "  #     print(f\"Class={label}-{dataset.classes[label]}\")\n",
        "  #     display(Image.fromarray((tensor.numpy().reshape(32, 32) * 255).astype(np.uint8), 'L'));\n",
        "  #   break\n",
        "  model = LinearClassifier(dataset.classes)\n",
        "  for images, classes in train_loader:\n",
        "    return model.train(images, classes)\n",
        "  \n",
        "my_test()"
      ],
      "metadata": {
        "id": "XEJxhWINqxo5",
        "outputId": "ad2177be-ce5d-4496-96a1-ae618beef99c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 354
        }
      },
      "execution_count": 371,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-371-4aef54d15b6b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mmy_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-371-4aef54d15b6b>\u001b[0m in \u001b[0;36mmy_test\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m   \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLinearClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclasses\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mmy_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-370-0bef668cf853>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, images, labels, learning_rate)\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;31m# images = torch.from_numpy(images)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mloss_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__weights\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-370-0bef668cf853>\u001b[0m in \u001b[0;36m__loss\u001b[0;34m(self, images, labels)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;31m# Calculate Multiclass SVM or Cross-entropy loss over a batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;31m# loss = self.__calc_svm_loss(images, labels, self.__weights)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__calc_mce_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;31m# Calculate gradients (dL/dW) and store it in dW\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-370-0bef668cf853>\u001b[0m in \u001b[0;36m__calc_mce_loss\u001b[0;34m(images, labels, weights)\u001b[0m\n\u001b[1;32m     81\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__calc_mce_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m     \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;31m# print(scores[:5])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (500x3073 and 1025x10)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "l0 = torch.from_numpy(np.array(\n",
        "    [0, 2]\n",
        "))\n",
        "\n",
        "k0 = torch.from_numpy(np.array(\n",
        "    [[1, 2, 3],\n",
        "    [4, 5, 6],\n",
        "    [7, 8, 9]]\n",
        "))\n",
        "\n",
        "k0[]"
      ],
      "metadata": {
        "id": "tAMA1UWj2jn5",
        "outputId": "3af3ba1d-bdc2-40a2-bb31-c63719a11e9d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 133
        }
      },
      "execution_count": 372,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-372-b654999da24b>\"\u001b[0;36m, line \u001b[0;32m11\u001b[0m\n\u001b[0;31m    k0[]\u001b[0m\n\u001b[0m       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def add_ones_to_tensor(tensor: Tensor):\n",
        "  return torch.from_numpy(np.append(tensor.numpy(), np.ones((tensor.shape[0], 1), dtype=np.float32), axis=1))\n",
        "\n",
        "class LinearClassifier(torch.nn.Module):\n",
        "  def __init__(self, input_dim=3073, output_dim=10):\n",
        "    super(LinearClassifier, self).__init__()\n",
        "    self.linear = torch.nn.Linear(input_dim, output_dim)\n",
        "  \n",
        "  def forward(self, x):\n",
        "    return self.linear(x)\n"
      ],
      "metadata": {
        "id": "k6jZTBXNI6IL"
      },
      "execution_count": 424,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7WdPIihtJMZ6"
      },
      "execution_count": 424,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = LinearClassifier()\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.25)\n",
        "\n",
        "all_loss = []\n",
        "\n",
        "for epoch in range(300):\n",
        "  print(f\"Epoch: {epoch}\")\n",
        "  for (images, classes) in train_loader:\n",
        "    images = add_ones_to_tensor(images.type(torch.float32) / (255))\n",
        "    output = model(images)\n",
        "\n",
        "    loss = criterion(output, classes.view(-1))\n",
        "    all_loss.append(loss.item())\n",
        "    loss.backward()\n",
        "\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "    break"
      ],
      "metadata": {
        "id": "LeO6o35BJXDj",
        "outputId": "f49f9837-ff29-43fb-a275-0712d6828868",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 425,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0\n",
            "Epoch: 1\n",
            "Epoch: 2\n",
            "Epoch: 3\n",
            "Epoch: 4\n",
            "Epoch: 5\n",
            "Epoch: 6\n",
            "Epoch: 7\n",
            "Epoch: 8\n",
            "Epoch: 9\n",
            "Epoch: 10\n",
            "Epoch: 11\n",
            "Epoch: 12\n",
            "Epoch: 13\n",
            "Epoch: 14\n",
            "Epoch: 15\n",
            "Epoch: 16\n",
            "Epoch: 17\n",
            "Epoch: 18\n",
            "Epoch: 19\n",
            "Epoch: 20\n",
            "Epoch: 21\n",
            "Epoch: 22\n",
            "Epoch: 23\n",
            "Epoch: 24\n",
            "Epoch: 25\n",
            "Epoch: 26\n",
            "Epoch: 27\n",
            "Epoch: 28\n",
            "Epoch: 29\n",
            "Epoch: 30\n",
            "Epoch: 31\n",
            "Epoch: 32\n",
            "Epoch: 33\n",
            "Epoch: 34\n",
            "Epoch: 35\n",
            "Epoch: 36\n",
            "Epoch: 37\n",
            "Epoch: 38\n",
            "Epoch: 39\n",
            "Epoch: 40\n",
            "Epoch: 41\n",
            "Epoch: 42\n",
            "Epoch: 43\n",
            "Epoch: 44\n",
            "Epoch: 45\n",
            "Epoch: 46\n",
            "Epoch: 47\n",
            "Epoch: 48\n",
            "Epoch: 49\n",
            "Epoch: 50\n",
            "Epoch: 51\n",
            "Epoch: 52\n",
            "Epoch: 53\n",
            "Epoch: 54\n",
            "Epoch: 55\n",
            "Epoch: 56\n",
            "Epoch: 57\n",
            "Epoch: 58\n",
            "Epoch: 59\n",
            "Epoch: 60\n",
            "Epoch: 61\n",
            "Epoch: 62\n",
            "Epoch: 63\n",
            "Epoch: 64\n",
            "Epoch: 65\n",
            "Epoch: 66\n",
            "Epoch: 67\n",
            "Epoch: 68\n",
            "Epoch: 69\n",
            "Epoch: 70\n",
            "Epoch: 71\n",
            "Epoch: 72\n",
            "Epoch: 73\n",
            "Epoch: 74\n",
            "Epoch: 75\n",
            "Epoch: 76\n",
            "Epoch: 77\n",
            "Epoch: 78\n",
            "Epoch: 79\n",
            "Epoch: 80\n",
            "Epoch: 81\n",
            "Epoch: 82\n",
            "Epoch: 83\n",
            "Epoch: 84\n",
            "Epoch: 85\n",
            "Epoch: 86\n",
            "Epoch: 87\n",
            "Epoch: 88\n",
            "Epoch: 89\n",
            "Epoch: 90\n",
            "Epoch: 91\n",
            "Epoch: 92\n",
            "Epoch: 93\n",
            "Epoch: 94\n",
            "Epoch: 95\n",
            "Epoch: 96\n",
            "Epoch: 97\n",
            "Epoch: 98\n",
            "Epoch: 99\n",
            "Epoch: 100\n",
            "Epoch: 101\n",
            "Epoch: 102\n",
            "Epoch: 103\n",
            "Epoch: 104\n",
            "Epoch: 105\n",
            "Epoch: 106\n",
            "Epoch: 107\n",
            "Epoch: 108\n",
            "Epoch: 109\n",
            "Epoch: 110\n",
            "Epoch: 111\n",
            "Epoch: 112\n",
            "Epoch: 113\n",
            "Epoch: 114\n",
            "Epoch: 115\n",
            "Epoch: 116\n",
            "Epoch: 117\n",
            "Epoch: 118\n",
            "Epoch: 119\n",
            "Epoch: 120\n",
            "Epoch: 121\n",
            "Epoch: 122\n",
            "Epoch: 123\n",
            "Epoch: 124\n",
            "Epoch: 125\n",
            "Epoch: 126\n",
            "Epoch: 127\n",
            "Epoch: 128\n",
            "Epoch: 129\n",
            "Epoch: 130\n",
            "Epoch: 131\n",
            "Epoch: 132\n",
            "Epoch: 133\n",
            "Epoch: 134\n",
            "Epoch: 135\n",
            "Epoch: 136\n",
            "Epoch: 137\n",
            "Epoch: 138\n",
            "Epoch: 139\n",
            "Epoch: 140\n",
            "Epoch: 141\n",
            "Epoch: 142\n",
            "Epoch: 143\n",
            "Epoch: 144\n",
            "Epoch: 145\n",
            "Epoch: 146\n",
            "Epoch: 147\n",
            "Epoch: 148\n",
            "Epoch: 149\n",
            "Epoch: 150\n",
            "Epoch: 151\n",
            "Epoch: 152\n",
            "Epoch: 153\n",
            "Epoch: 154\n",
            "Epoch: 155\n",
            "Epoch: 156\n",
            "Epoch: 157\n",
            "Epoch: 158\n",
            "Epoch: 159\n",
            "Epoch: 160\n",
            "Epoch: 161\n",
            "Epoch: 162\n",
            "Epoch: 163\n",
            "Epoch: 164\n",
            "Epoch: 165\n",
            "Epoch: 166\n",
            "Epoch: 167\n",
            "Epoch: 168\n",
            "Epoch: 169\n",
            "Epoch: 170\n",
            "Epoch: 171\n",
            "Epoch: 172\n",
            "Epoch: 173\n",
            "Epoch: 174\n",
            "Epoch: 175\n",
            "Epoch: 176\n",
            "Epoch: 177\n",
            "Epoch: 178\n",
            "Epoch: 179\n",
            "Epoch: 180\n",
            "Epoch: 181\n",
            "Epoch: 182\n",
            "Epoch: 183\n",
            "Epoch: 184\n",
            "Epoch: 185\n",
            "Epoch: 186\n",
            "Epoch: 187\n",
            "Epoch: 188\n",
            "Epoch: 189\n",
            "Epoch: 190\n",
            "Epoch: 191\n",
            "Epoch: 192\n",
            "Epoch: 193\n",
            "Epoch: 194\n",
            "Epoch: 195\n",
            "Epoch: 196\n",
            "Epoch: 197\n",
            "Epoch: 198\n",
            "Epoch: 199\n",
            "Epoch: 200\n",
            "Epoch: 201\n",
            "Epoch: 202\n",
            "Epoch: 203\n",
            "Epoch: 204\n",
            "Epoch: 205\n",
            "Epoch: 206\n",
            "Epoch: 207\n",
            "Epoch: 208\n",
            "Epoch: 209\n",
            "Epoch: 210\n",
            "Epoch: 211\n",
            "Epoch: 212\n",
            "Epoch: 213\n",
            "Epoch: 214\n",
            "Epoch: 215\n",
            "Epoch: 216\n",
            "Epoch: 217\n",
            "Epoch: 218\n",
            "Epoch: 219\n",
            "Epoch: 220\n",
            "Epoch: 221\n",
            "Epoch: 222\n",
            "Epoch: 223\n",
            "Epoch: 224\n",
            "Epoch: 225\n",
            "Epoch: 226\n",
            "Epoch: 227\n",
            "Epoch: 228\n",
            "Epoch: 229\n",
            "Epoch: 230\n",
            "Epoch: 231\n",
            "Epoch: 232\n",
            "Epoch: 233\n",
            "Epoch: 234\n",
            "Epoch: 235\n",
            "Epoch: 236\n",
            "Epoch: 237\n",
            "Epoch: 238\n",
            "Epoch: 239\n",
            "Epoch: 240\n",
            "Epoch: 241\n",
            "Epoch: 242\n",
            "Epoch: 243\n",
            "Epoch: 244\n",
            "Epoch: 245\n",
            "Epoch: 246\n",
            "Epoch: 247\n",
            "Epoch: 248\n",
            "Epoch: 249\n",
            "Epoch: 250\n",
            "Epoch: 251\n",
            "Epoch: 252\n",
            "Epoch: 253\n",
            "Epoch: 254\n",
            "Epoch: 255\n",
            "Epoch: 256\n",
            "Epoch: 257\n",
            "Epoch: 258\n",
            "Epoch: 259\n",
            "Epoch: 260\n",
            "Epoch: 261\n",
            "Epoch: 262\n",
            "Epoch: 263\n",
            "Epoch: 264\n",
            "Epoch: 265\n",
            "Epoch: 266\n",
            "Epoch: 267\n",
            "Epoch: 268\n",
            "Epoch: 269\n",
            "Epoch: 270\n",
            "Epoch: 271\n",
            "Epoch: 272\n",
            "Epoch: 273\n",
            "Epoch: 274\n",
            "Epoch: 275\n",
            "Epoch: 276\n",
            "Epoch: 277\n",
            "Epoch: 278\n",
            "Epoch: 279\n",
            "Epoch: 280\n",
            "Epoch: 281\n",
            "Epoch: 282\n",
            "Epoch: 283\n",
            "Epoch: 284\n",
            "Epoch: 285\n",
            "Epoch: 286\n",
            "Epoch: 287\n",
            "Epoch: 288\n",
            "Epoch: 289\n",
            "Epoch: 290\n",
            "Epoch: 291\n",
            "Epoch: 292\n",
            "Epoch: 293\n",
            "Epoch: 294\n",
            "Epoch: 295\n",
            "Epoch: 296\n",
            "Epoch: 297\n",
            "Epoch: 298\n",
            "Epoch: 299\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for images, labels in val_loader:\n",
        "  images = add_ones_to_tensor(images.type(torch.float32) / 255)\n",
        "  probabilities = model.forward(images)\n",
        "  print(probabilities.shape)\n",
        "  pred = torch.argmax(probabilities, dim=1)\n",
        "  print(pred.shape)\n",
        "  acc = (pred.numpy() == labels.numpy()).sum() / pred.shape[0]\n",
        "  print(f\"accuracy = {acc}\")\n",
        "  "
      ],
      "metadata": {
        "id": "Yx2EV7uLK3k2",
        "outputId": "baf95931-06a2-459b-f6c7-9129ab1f072e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 426,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10000, 10])\n",
            "torch.Size([10000])\n",
            "accuracy = 0.3089\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(all_loss)"
      ],
      "metadata": {
        "id": "VBaKOvP9LNIV",
        "outputId": "66ff872f-548b-4adb-f143-975956e51b2f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        }
      },
      "execution_count": 427,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7fd4fc26bb50>]"
            ]
          },
          "metadata": {},
          "execution_count": 427
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for images, labels in val_loader:\n",
        "  images = "
      ],
      "metadata": {
        "id": "X5pPE-80Swh1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cyVPgrr5xjhU"
      },
      "source": [
        "# Train model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v5zVN1kHd43W"
      },
      "source": [
        "## Function for accuracy checking\n",
        "\n",
        "Don't change this code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dzhRClCsdzJw"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "def validate(model,dataloader):\n",
        "  y_predicted = np.array([])\n",
        "  y_gtrue = np.array([])\n",
        "  for images, class_nums in dataloader:\n",
        "    index = model.predict(images.numpy())\n",
        "    y_predicted = np.append(y_predicted,index) \n",
        "    y_gtrue = np.append(y_gtrue,class_nums.numpy()) \n",
        "  return accuracy_score(y_gtrue, y_predicted)"
      ],
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HQr1qYUlxq7X"
      },
      "source": [
        "## Train loop\n",
        "Let's train our model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "phcDEj7OdpGS",
        "outputId": "c3b38565-635a-4e61-c4b1-3e652e96eba1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "\n",
        "model = LinearClassifier(dataset.classes)\n",
        "best_accuracy = 0\n",
        "for epoch in range(25):\n",
        "  print(f\"Starting epoch={epoch}\")\n",
        "  for images, class_nums in train_loader:\n",
        "    loss = model.train(images, class_nums)\n",
        "  accuracy = validate(model,val_loader)\n",
        "  if best_accuracy < accuracy:\n",
        "    best_accuracy = accuracy\n",
        "  print(f\"Epoch {epoch} Loss: {loss}, Accuracy:{accuracy}\")\n",
        "\n",
        "print(f\"Best accuracy is {best_accuracy}\")"
      ],
      "execution_count": 224,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting epoch=0\n",
            "Epoch 0 Loss: 2.2402150630950928, Accuracy:0.24\n",
            "Starting epoch=1\n",
            "Epoch 1 Loss: 2.1346638202667236, Accuracy:0.225\n",
            "Starting epoch=2\n",
            "Epoch 2 Loss: 1.9985244274139404, Accuracy:0.217\n",
            "Starting epoch=3\n",
            "Epoch 3 Loss: 2.019589900970459, Accuracy:0.197\n",
            "Starting epoch=4\n",
            "Epoch 4 Loss: 2.340911388397217, Accuracy:0.19\n",
            "Starting epoch=5\n",
            "Epoch 5 Loss: 2.1221439838409424, Accuracy:0.221\n",
            "Starting epoch=6\n",
            "Epoch 6 Loss: 2.505711793899536, Accuracy:0.207\n",
            "Starting epoch=7\n",
            "Epoch 7 Loss: 2.397432327270508, Accuracy:0.218\n",
            "Starting epoch=8\n",
            "Epoch 8 Loss: 2.68744158744812, Accuracy:0.191\n",
            "Starting epoch=9\n",
            "Epoch 9 Loss: 2.6366589069366455, Accuracy:0.204\n",
            "Starting epoch=10\n",
            "Epoch 10 Loss: 4.097376346588135, Accuracy:0.191\n",
            "Starting epoch=11\n",
            "Epoch 11 Loss: 3.4222781658172607, Accuracy:0.182\n",
            "Starting epoch=12\n",
            "Epoch 12 Loss: 3.5858991146087646, Accuracy:0.155\n",
            "Starting epoch=13\n",
            "Epoch 13 Loss: 2.3892457485198975, Accuracy:0.206\n",
            "Starting epoch=14\n",
            "Epoch 14 Loss: 2.4798598289489746, Accuracy:0.193\n",
            "Starting epoch=15\n",
            "Epoch 15 Loss: 3.02689790725708, Accuracy:0.193\n",
            "Starting epoch=16\n",
            "Epoch 16 Loss: 3.684767007827759, Accuracy:0.19\n",
            "Starting epoch=17\n",
            "Epoch 17 Loss: 2.413422107696533, Accuracy:0.188\n",
            "Starting epoch=18\n",
            "Epoch 18 Loss: 4.783426284790039, Accuracy:0.158\n",
            "Starting epoch=19\n",
            "Epoch 19 Loss: 4.3332014083862305, Accuracy:0.184\n",
            "Starting epoch=20\n",
            "Epoch 20 Loss: 4.080665588378906, Accuracy:0.195\n",
            "Starting epoch=21\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-224-5c0362b6396a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Starting epoch={epoch}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_nums\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_nums\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m   \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mval_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mbest_accuracy\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-217-0bef668cf853>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, images, labels, learning_rate)\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;31m# images = torch.from_numpy(images)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mloss_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__weights\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-217-0bef668cf853>\u001b[0m in \u001b[0;36m__loss\u001b[0;34m(self, images, labels)\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;31m# Calculate gradients (dL/dW) and store it in dW\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;31m# grad = self.__calc_numeric_grad(partial(self.__calc_svm_loss, images, labels), loss)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m     \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__w_unnormalized\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    394\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 396\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    173\u001b[0m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[1;32m    174\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m def grad(\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D4tIFR5bwZFi"
      },
      "source": [
        "# Check model on test dataset\n",
        "\n",
        "You must get accuracy above 0.35\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MM0pWYJlwibm"
      },
      "source": [
        "test_dataset = datasets.CIFAR10(\"content\",\n",
        "                           train=False,\n",
        "                           transform = transform, # Transforms stay the same\n",
        "                           download=True)\n",
        "\n",
        "test_loader = DataLoader(test_dataset, batch_size = batch_size)\n",
        "\n",
        "accuracy = validate(model,test_loader)\n",
        "print(f\"Accuracy on test:{accuracy}\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZsImxpggG8bH"
      },
      "source": [
        "# Place for brief conclusion\n",
        "Feel free to describe troubles here.\n",
        "\n",
        "\n",
        "...\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJ13OmfCEb1w"
      },
      "source": [
        "# Ideas for extra work\n",
        "\n",
        "- Implenment CrossEntropyLoss function\n",
        "- Implement bias trick\n",
        "- Add regularization to SVM loss\n",
        "- Find best learning rate and regularization strength using Cross-Validation\n",
        "- Normalize data\n",
        "\n",
        "\n",
        " "
      ]
    }
  ]
}